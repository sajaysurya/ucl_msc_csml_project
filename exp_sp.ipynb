{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "experiment - shortest path MDP\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import agent\n",
    "import room as fl\n",
    "import utils\n",
    "\n",
    "# pretify\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data structures to store the results\n",
    "'''\n",
    "log = pd.DataFrame(columns=['algorithm',\n",
    "                           'average-reward/time-step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "experiment setting\n",
    "'''\n",
    "# make the game to get details\n",
    "game = fl.make_game()\n",
    "mobs = game.its_showtime()\n",
    "\n",
    "# parameters\n",
    "num_experiments = 10\n",
    "world_param = fl.get_params()\n",
    "num_states = world_param['num_states']\n",
    "num_actions = world_param['num_actions']\n",
    "len_episodes = 100\n",
    "discount = 0.9\n",
    "# reward function\n",
    "reward = np.zeros((num_states, num_actions))\n",
    "goal_loc = utils.get_location(mobs, 'X')\n",
    "# in all the following cases, we have 100 -> enters the goal state\n",
    "reward[goal_loc+1, 0] = 100\n",
    "reward[goal_loc-1, 1] = 100\n",
    "reward[goal_loc+11, 2] = 100\n",
    "reward[goal_loc-11, 3] = 100\n",
    "# start distribution - always current start location\n",
    "start_loc = utils.get_location(mobs, 'O')\n",
    "start_dist = np.zeros(num_states)\n",
    "start_dist[start_loc] = 1\n",
    "\n",
    "# transition distribution (just alpha counts)\n",
    "alpha = np.zeros((num_states, num_states, num_actions))\n",
    "# go through all states\n",
    "for state in range(num_states):\n",
    "    # change if it is not a wall\n",
    "    if not utils.is_wall(state, mobs):\n",
    "        # if there is no wall, give 1\n",
    "        if not utils.is_wall(state-1, mobs):\n",
    "            alpha[state-1, state, 0] = 1\n",
    "        else:  # stay if there is wall\n",
    "            alpha[state, state, 0] = 1\n",
    "\n",
    "        if not utils.is_wall(state+1, mobs):\n",
    "            alpha[state+1, state, 1] = 1\n",
    "        else:  # stay if there is wall\n",
    "            alpha[state, state, 1] = 1\n",
    "\n",
    "        if not utils.is_wall(state-11, mobs):\n",
    "            alpha[state-11, state, 2] = 1\n",
    "        else:  # stay if there is wall\n",
    "            alpha[state, state, 2] = 1\n",
    "\n",
    "        if not utils.is_wall(state+11, mobs):\n",
    "            alpha[state+11, state, 3] = 1\n",
    "        else:  # stay if there is wall\n",
    "            alpha[state, state, 3] = 1\n",
    "    else: # if inside wall, you will stay inside wall\n",
    "        alpha[state, state, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5adc890ee814cf581829913d621ad0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 3 0 0]\n",
      " [0 0 1 2 0 0 0 1 3 0 0]\n",
      " [0 0 0 2 0 0 0 1 0 0 0]\n",
      " [0 0 1 3 0 0 0 1 2 0 0]\n",
      " [0 1 1 1 1 1 1 1 2 0 0]\n",
      " [0 0 1 1 1 1 1 1 2 0 0]\n",
      " [0 0 0 1 1 1 1 1 2 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using Policy Iteration - 10 Iterations\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "# 0 - left\n",
    "# 1 - right\n",
    "# 2 - up\n",
    "# 3 - down\n",
    "det_policy = np.zeros(num_states)\n",
    "det_policy[24] = 3\n",
    "det_policy[35] = 3\n",
    "det_policy[46] = 3\n",
    "det_policy[57] = 1\n",
    "det_policy[58] = 1\n",
    "det_policy[59] = 1\n",
    "det_policy[60] = 1\n",
    "det_policy[61] = 1\n",
    "det_policy[62] = 1\n",
    "det_policy[63] = 2\n",
    "det_policy[52] = 2\n",
    "det_policy[41] = 2\n",
    "det_policy[30] = 2\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.PoliQ(num_actions,\n",
    "                      num_states,\n",
    "                      discount,\n",
    "                      len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(det_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=10)  # 0 -> till convergence\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-PI-10\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "    \n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy, 2), (9,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69becfd293634c899ebf0bf25008efa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.23 0.18 0.25 0.24 0.23 0.21 0.13 0.   0.26 0.25]\n",
      " [0.25 0.22 0.03 0.33 0.25 0.25 0.25 0.2  0.04 0.37 0.25]\n",
      " [0.25 0.21 0.04 0.31 0.25 0.25 0.25 0.11 0.   0.9  0.25]\n",
      " [0.25 0.21 0.04 0.25 0.25 0.25 0.25 0.14 0.03 0.5  0.25]\n",
      " [0.25 0.23 0.03 0.02 0.03 0.02 0.03 0.03 0.02 0.37 0.25]\n",
      " [0.25 0.23 0.15 0.16 0.16 0.17 0.17 0.18 0.22 0.26 0.25]\n",
      " [0.25 0.24 0.22 0.22 0.22 0.22 0.22 0.23 0.23 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.26 0.19 0.23 0.27 0.29 0.31 0.3  0.   0.21 0.25]\n",
      " [0.25 0.36 0.02 0.22 0.25 0.25 0.25 0.38 0.04 0.19 0.25]\n",
      " [0.25 0.38 0.02 0.21 0.25 0.25 0.25 0.89 0.   0.1  0.25]\n",
      " [0.25 0.4  0.01 0.22 0.25 0.25 0.25 0.46 0.03 0.13 0.25]\n",
      " [0.25 0.46 0.93 0.91 0.93 0.93 0.93 0.93 0.03 0.22 0.25]\n",
      " [0.25 0.28 0.3  0.3  0.3  0.3  0.29 0.27 0.17 0.23 0.25]\n",
      " [0.25 0.26 0.26 0.26 0.26 0.25 0.25 0.24 0.23 0.24 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.23 0.23 0.23 0.24 0.24 0.24 0.23 0.13 0.21 0.25]\n",
      " [0.25 0.14 0.04 0.14 0.25 0.25 0.25 0.02 0.04 0.05 0.25]\n",
      " [0.25 0.14 0.02 0.13 0.25 0.25 0.25 0.   0.49 0.   0.25]\n",
      " [0.25 0.15 0.03 0.06 0.25 0.25 0.25 0.33 0.91 0.36 0.25]\n",
      " [0.25 0.2  0.02 0.03 0.01 0.01 0.01 0.   0.92 0.32 0.25]\n",
      " [0.25 0.28 0.41 0.43 0.45 0.45 0.46 0.49 0.53 0.31 0.25]\n",
      " [0.25 0.26 0.28 0.29 0.29 0.29 0.29 0.3  0.3  0.27 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.28 0.41 0.28 0.24 0.24 0.24 0.33 0.86 0.31 0.25]\n",
      " [0.25 0.28 0.92 0.31 0.25 0.25 0.25 0.4  0.88 0.39 0.25]\n",
      " [0.25 0.27 0.92 0.35 0.25 0.25 0.25 0.   0.51 0.   0.25]\n",
      " [0.25 0.24 0.93 0.47 0.25 0.25 0.25 0.07 0.03 0.   0.25]\n",
      " [0.25 0.11 0.03 0.03 0.04 0.03 0.03 0.04 0.04 0.09 0.25]\n",
      " [0.25 0.2  0.14 0.11 0.09 0.08 0.08 0.07 0.08 0.19 0.25]\n",
      " [0.25 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\uparrow$     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using GD - 100 iterations, step_size=0.001\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MDP_GD(num_actions,\n",
    "                        num_states,\n",
    "                        discount,\n",
    "                        len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=100, step_size=0.001)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-GD-100-0.001\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974ee4dbdcf45e6b28489d34d418bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.05 0.06 0.06 0.08 0.08 0.05 0.05 0.4  0.25]\n",
      " [0.25 0.   0.03 0.07 0.25 0.25 0.25 0.03 0.03 0.42 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.03 0.17 0.87 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.   0.03 0.52 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.46 0.25]\n",
      " [0.25 0.22 0.   0.   0.   0.   0.   0.   0.   0.28 0.25]\n",
      " [0.25 0.24 0.2  0.18 0.17 0.17 0.18 0.19 0.21 0.24 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.99 0.88 0.84 0.88 0.85 0.84 0.32 0.04 0.   0.25]\n",
      " [0.25 0.52 0.48 0.03 0.25 0.25 0.25 0.42 0.03 0.   0.25]\n",
      " [0.25 0.5  0.08 0.   0.25 0.25 0.25 0.88 0.22 0.03 0.25]\n",
      " [0.25 0.61 0.23 0.   0.25 0.25 0.25 0.52 0.03 0.   0.25]\n",
      " [0.25 1.   1.   1.   1.   1.   1.   0.6  0.   0.01 0.25]\n",
      " [0.25 0.31 0.52 0.54 0.52 0.5  0.46 0.33 0.   0.23 0.25]\n",
      " [0.25 0.26 0.28 0.28 0.28 0.27 0.26 0.24 0.21 0.23 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.03 0.03 0.03 0.03 0.04 0.03 0.03 0.   0.25]\n",
      " [0.25 0.05 0.48 0.89 0.25 0.25 0.25 0.06 0.03 0.   0.25]\n",
      " [0.25 0.01 0.   0.02 0.25 0.25 0.25 0.05 0.17 0.05 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.48 0.91 0.48 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.4  1.   0.53 0.25]\n",
      " [0.25 0.29 0.48 0.46 0.48 0.49 0.54 0.67 1.   0.35 0.25]\n",
      " [0.25 0.27 0.29 0.31 0.33 0.33 0.34 0.35 0.35 0.29 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.05 0.06 0.03 0.03 0.04 0.6  0.88 0.59 0.25]\n",
      " [0.25 0.43 0.   0.01 0.25 0.25 0.25 0.49 0.91 0.58 0.25]\n",
      " [0.25 0.49 0.92 0.98 0.25 0.25 0.25 0.05 0.43 0.05 0.25]\n",
      " [0.25 0.38 0.77 1.   0.25 0.25 0.25 0.   0.03 0.   0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.17 0.   0.   0.   0.   0.   0.   0.   0.14 0.25]\n",
      " [0.25 0.24 0.23 0.23 0.22 0.22 0.22 0.22 0.23 0.23 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\rightarrow$  & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using GD - 100 iterations, step_size=0.01\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MDP_GD(num_actions,\n",
    "                        num_states,\n",
    "                        discount,\n",
    "                        len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=100, step_size=0.01)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-GD-100-0.01\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bc4ccbd13b47b5875542a5ed274889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.02 0.05 0.05 0.06 0.06 0.06 0.05 0.   0.   0.25]\n",
      " [0.25 0.02 0.04 0.   0.25 0.25 0.25 0.02 0.04 0.46 0.25]\n",
      " [0.25 0.   0.01 0.02 0.25 0.25 0.25 0.03 0.29 0.89 0.25]\n",
      " [0.25 0.12 0.2  0.   0.25 0.25 0.25 0.01 0.05 0.43 0.25]\n",
      " [0.25 0.18 0.23 0.   0.02 0.04 0.02 0.   0.   0.14 0.25]\n",
      " [0.25 0.22 0.   0.   0.   0.   0.   0.   0.   0.28 0.25]\n",
      " [0.25 0.23 0.18 0.15 0.14 0.14 0.14 0.14 0.16 0.23 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.91 0.88 0.88 0.88 0.88 0.88 0.29 0.   0.02 0.25]\n",
      " [0.25 0.51 0.41 0.02 0.25 0.25 0.25 0.48 0.04 0.   0.25]\n",
      " [0.25 0.39 0.46 0.01 0.25 0.25 0.25 0.9  0.28 0.03 0.25]\n",
      " [0.25 0.41 0.21 0.   0.25 0.25 0.25 0.51 0.05 0.01 0.25]\n",
      " [0.25 0.62 0.4  0.5  0.58 0.57 0.6  0.59 0.   0.   0.25]\n",
      " [0.25 0.33 0.71 1.   1.   0.94 0.74 0.46 0.   0.19 0.25]\n",
      " [0.25 0.28 0.31 0.32 0.32 0.32 0.3  0.27 0.2  0.22 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.02 0.02 0.02 0.03 0.03 0.03 0.03 0.   0.02 0.25]\n",
      " [0.25 0.44 0.5  0.97 0.25 0.25 0.25 0.01 0.05 0.   0.25]\n",
      " [0.25 0.61 0.51 0.96 0.25 0.25 0.25 0.04 0.23 0.04 0.25]\n",
      " [0.25 0.28 0.2  0.1  0.25 0.25 0.25 0.48 0.87 0.53 0.25]\n",
      " [0.25 0.12 0.09 0.18 0.   0.   0.   0.33 1.   0.85 0.25]\n",
      " [0.25 0.26 0.29 0.   0.   0.06 0.25 0.54 0.99 0.53 0.25]\n",
      " [0.25 0.26 0.28 0.3  0.31 0.33 0.34 0.37 0.42 0.32 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.04 0.05 0.05 0.03 0.03 0.03 0.63 1.   0.97 0.25]\n",
      " [0.25 0.03 0.04 0.01 0.25 0.25 0.25 0.48 0.87 0.54 0.25]\n",
      " [0.25 0.   0.02 0.01 0.25 0.25 0.25 0.04 0.2  0.04 0.25]\n",
      " [0.25 0.19 0.39 0.9  0.25 0.25 0.25 0.01 0.04 0.02 0.25]\n",
      " [0.25 0.09 0.28 0.32 0.4  0.39 0.37 0.08 0.   0.   0.25]\n",
      " [0.25 0.19 0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.23 0.23 0.23 0.22 0.22 0.22 0.21 0.21 0.22 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\rightarrow$  & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using GD - 100 iterations, step_size=0.1\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MDP_GD(num_actions,\n",
    "                        num_states,\n",
    "                        discount,\n",
    "                        len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=100, step_size=0.1)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-GD-100-0.1\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24bb839781443babeb4d55c1af2805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.03 0.05 0.06 0.06 0.06 0.05 0.05 0.   0.25]\n",
      " [0.25 0.   0.03 0.05 0.25 0.25 0.25 0.   0.04 0.52 0.25]\n",
      " [0.25 0.   0.25 0.04 0.25 0.25 0.25 0.03 0.3  0.87 0.25]\n",
      " [0.25 0.   0.42 0.   0.25 0.25 0.25 0.01 0.02 0.02 0.25]\n",
      " [0.25 0.   0.35 0.2  0.28 0.26 0.25 0.18 0.14 0.   0.25]\n",
      " [0.25 0.11 0.1  0.03 0.07 0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.18 0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.99 0.95 0.88 0.88 0.88 0.89 0.34 0.05 0.   0.25]\n",
      " [0.25 0.52 0.43 0.02 0.25 0.25 0.25 0.58 0.04 0.02 0.25]\n",
      " [0.25 0.   0.38 0.02 0.25 0.25 0.25 0.87 0.33 0.03 0.25]\n",
      " [0.25 0.36 0.19 0.   0.25 0.25 0.25 0.03 0.02 0.01 0.25]\n",
      " [0.25 0.66 0.12 0.12 0.13 0.13 0.14 0.15 0.32 0.   0.25]\n",
      " [0.25 0.44 0.23 0.21 0.23 0.29 0.47 0.83 0.55 0.   0.25]\n",
      " [0.25 0.3  0.39 0.48 0.6  0.76 1.   0.65 0.32 0.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.02 0.02 0.03 0.03 0.03 0.03 0.02 0.   0.25]\n",
      " [0.25 0.45 0.5  0.88 0.25 0.25 0.25 0.   0.04 0.03 0.25]\n",
      " [0.25 1.   0.37 0.92 0.25 0.25 0.25 0.05 0.35 0.05 0.25]\n",
      " [0.25 0.63 0.28 0.99 0.25 0.25 0.25 0.94 0.92 0.95 0.25]\n",
      " [0.25 0.19 0.19 0.34 0.1  0.1  0.1  0.19 0.16 1.   0.25]\n",
      " [0.25 0.44 0.53 0.56 0.48 0.45 0.31 0.16 0.45 1.   0.25]\n",
      " [0.25 0.33 0.45 0.41 0.32 0.21 0.   0.35 0.68 1.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.   0.05 0.03 0.03 0.03 0.58 0.88 0.99 0.25]\n",
      " [0.25 0.02 0.04 0.05 0.25 0.25 0.25 0.42 0.87 0.43 0.25]\n",
      " [0.25 0.   0.   0.03 0.25 0.25 0.25 0.05 0.03 0.05 0.25]\n",
      " [0.25 0.   0.11 0.   0.25 0.25 0.25 0.02 0.04 0.02 0.25]\n",
      " [0.25 0.15 0.35 0.35 0.49 0.51 0.51 0.48 0.38 0.   0.25]\n",
      " [0.25 0.   0.14 0.2  0.23 0.26 0.22 0.   0.   0.   0.25]\n",
      " [0.25 0.18 0.16 0.11 0.08 0.03 0.   0.   0.   0.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\rightarrow$  & $\\downarrow$   &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using GD - 100 iterations, step_size=1\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MDP_GD(num_actions,\n",
    "                        num_states,\n",
    "                        discount,\n",
    "                        len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=100, step_size=1)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-GD-100-1\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2aeb14d21c446bbb985b4f8f362a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.09 0.02 0.04 0.01 0.01 0.01 0.01 0.04 0.16 0.25]\n",
      " [0.25 0.09 0.   0.27 0.25 0.25 0.25 0.09 0.01 0.2  0.25]\n",
      " [0.25 0.09 0.   0.15 0.25 0.25 0.25 0.09 0.13 0.87 0.25]\n",
      " [0.25 0.09 0.   0.06 0.25 0.25 0.25 0.09 0.   0.65 0.25]\n",
      " [0.25 0.09 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.6  0.25]\n",
      " [0.25 0.09 0.01 0.02 0.02 0.02 0.02 0.03 0.03 0.35 0.25]\n",
      " [0.25 0.09 0.02 0.02 0.02 0.02 0.03 0.03 0.04 0.24 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.38 0.2  0.78 0.82 0.82 0.82 0.18 0.05 0.09 0.25]\n",
      " [0.25 0.58 0.01 0.09 0.25 0.25 0.25 0.2  0.01 0.09 0.25]\n",
      " [0.25 0.6  0.02 0.09 0.25 0.25 0.25 0.87 0.13 0.09 0.25]\n",
      " [0.25 0.65 0.04 0.09 0.25 0.25 0.25 0.63 0.   0.09 0.25]\n",
      " [0.25 0.86 0.99 0.99 0.96 0.96 0.96 0.94 0.   0.09 0.25]\n",
      " [0.25 0.51 0.32 0.32 0.31 0.31 0.29 0.24 0.02 0.09 0.25]\n",
      " [0.25 0.39 0.32 0.31 0.3  0.29 0.26 0.19 0.03 0.09 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.25]\n",
      " [0.25 0.02 0.   0.08 0.25 0.25 0.25 0.01 0.03 0.01 0.25]\n",
      " [0.25 0.02 0.01 0.01 0.25 0.25 0.25 0.01 0.37 0.01 0.25]\n",
      " [0.25 0.03 0.01 0.01 0.25 0.25 0.25 0.23 0.99 0.24 0.25]\n",
      " [0.25 0.03 0.01 0.   0.03 0.03 0.03 0.05 0.99 0.3  0.25]\n",
      " [0.25 0.39 0.65 0.65 0.65 0.66 0.67 0.73 0.94 0.55 0.25]\n",
      " [0.25 0.44 0.57 0.58 0.58 0.6  0.62 0.69 0.84 0.58 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.45 0.69 0.1  0.09 0.09 0.09 0.73 0.82 0.67 0.25]\n",
      " [0.25 0.31 0.98 0.57 0.25 0.25 0.25 0.7  0.95 0.7  0.25]\n",
      " [0.25 0.29 0.97 0.75 0.25 0.25 0.25 0.03 0.36 0.03 0.25]\n",
      " [0.25 0.23 0.95 0.84 0.25 0.25 0.25 0.05 0.01 0.03 0.25]\n",
      " [0.25 0.02 0.   0.   0.   0.   0.   0.   0.   0.01 0.25]\n",
      " [0.25 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.25]\n",
      " [0.25 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\downarrow$   &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\uparrow$     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using EM - 10 iterations\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MLEM(num_actions,\n",
    "                      num_states,\n",
    "                      discount,\n",
    "                      len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=10)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-EM-10\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ef6b6ffbcd4fa48dda3744202afcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.19 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.   0.   0.24 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.   0.12 1.   0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.   0.   0.75 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.7  0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.29 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 1.   1.   1.   1.   1.   1.   0.19 0.   0.   0.25]\n",
      " [0.25 0.38 0.5  0.   0.25 0.25 0.25 0.24 0.   0.   0.25]\n",
      " [0.25 0.7  0.01 0.   0.25 0.25 0.25 1.   0.12 0.   0.25]\n",
      " [0.25 0.76 0.03 0.   0.25 0.25 0.25 0.75 0.   0.   0.25]\n",
      " [0.25 1.   1.   1.   1.   1.   1.   0.96 0.   0.   0.25]\n",
      " [0.25 0.6  0.32 0.32 0.32 0.32 0.3  0.24 0.   0.   0.25]\n",
      " [0.25 0.47 0.32 0.32 0.32 0.3  0.27 0.19 0.   0.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.62 0.5  1.   0.25 0.25 0.25 0.   0.   0.   0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.   0.38 0.   0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.25 1.   0.25 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.04 1.   0.3  0.25]\n",
      " [0.25 0.4  0.68 0.68 0.68 0.68 0.7  0.76 1.   0.59 0.25]\n",
      " [0.25 0.53 0.68 0.68 0.68 0.7  0.73 0.81 1.   0.71 0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.81 1.   0.81 0.25]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.25 0.76 1.   0.76 0.25]\n",
      " [0.25 0.3  0.99 1.   0.25 0.25 0.25 0.   0.38 0.   0.25]\n",
      " [0.25 0.24 0.97 1.   0.25 0.25 0.25 0.   0.   0.   0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25]\n",
      " [0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]]\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\hline\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\rightarrow$  & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                & $\\uparrow$     &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\downarrow$   &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\rightarrow$  & \\texttt{G}     &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ &                &                &                &                &                &                &                &                &                & $\\blacksquare$ \\\\\n",
      " $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ & $\\blacksquare$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solving MDP using EM - 75 iterations\n",
    "'''\n",
    "# initialize policy to local minima\n",
    "sto_policy = np.ones((num_actions, num_states))\n",
    "sto_policy[3, 24] = 10\n",
    "sto_policy[3, 35] = 10\n",
    "sto_policy[3, 46] = 10\n",
    "sto_policy[1, 57] = 10\n",
    "sto_policy[1, 58] = 10\n",
    "sto_policy[1, 59] = 10\n",
    "sto_policy[1, 60] = 10\n",
    "sto_policy[1, 61] = 10\n",
    "sto_policy[1, 62] = 10\n",
    "sto_policy[2, 63] = 10\n",
    "sto_policy[2, 52] = 10\n",
    "sto_policy[2, 41] = 10\n",
    "sto_policy[2, 30] = 10\n",
    "sto_policy /= np.sum(sto_policy, axis=0)  # normalize to make dist.\n",
    "\n",
    "for _ in tqdm(range(num_experiments)):\n",
    "    # initialize total_reward\n",
    "    total_reward = 0\n",
    "    # create agent\n",
    "    bond = agent.MLEM(num_actions,\n",
    "                      num_states,\n",
    "                      discount,\n",
    "                      len_episodes)\n",
    "    # set transition distribution\n",
    "    bond.alpha = np.copy(alpha)\n",
    "    bond.update_theta()\n",
    "    # set reward details\n",
    "    bond.reward = np.copy(reward)\n",
    "    # set start distribution\n",
    "    bond.start_dist = np.copy(start_dist)\n",
    "    # use the same initial suboptimal policy\n",
    "    bond.policy = np.copy(sto_policy)\n",
    "\n",
    "    # perform inference\n",
    "    bond.learn(niter=75)\n",
    "\n",
    "    # make game for evaluation\n",
    "    game = fl.make_game()\n",
    "    obs = game.its_showtime()\n",
    "    for _ in range(len_episodes):\n",
    "        action = bond.play(utils.get_location(obs, char='O'))\n",
    "        obs = game.play(action)\n",
    "        if not obs[1] is None:\n",
    "            total_reward += obs[1]        \n",
    "    # add average reward to log\n",
    "    log=log.append(pd.DataFrame(\n",
    "        {'algorithm': \"MDP-EM-75\",\n",
    "         'average-reward/time-step': total_reward/len_episodes},\n",
    "        index=[log.size+1]))\n",
    "    # quit game\n",
    "    game.play(5)\n",
    "\n",
    "# print final policy\n",
    "print(np.reshape(np.round(bond.policy[0], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[1], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[2], 2), (9,11)))\n",
    "print(np.reshape(np.round(bond.policy[3], 2), (9,11)))\n",
    "utils.make_latex_table(mobs, bond.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "save the reward values\n",
    "'''\n",
    "log.to_pickle(\"exp_sp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x378 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "code for plotting a good graph\n",
    "'''\n",
    "sns.set(rc={'figure.figsize':(8,5.25)})\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(y=\"algorithm\",\n",
    "              x=\"average-reward/time-step\",\n",
    "              data=log,\n",
    "              ci=90)\n",
    "plt.savefig('exp_sp_result.png', dpi=600, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
